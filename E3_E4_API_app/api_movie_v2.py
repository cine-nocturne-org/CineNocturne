# -----------------------------
# Imports n√©cessaires
# -----------------------------
import os
import io
import csv
import logging
import random
import unicodedata
from datetime import datetime
from typing import List
import re

import pandas as pd
import numpy as np
import joblib
import mlflow
from sklearn.metrics.pairwise import cosine_similarity
from fastapi import FastAPI, HTTPException, Depends, Query
from fastapi.security import HTTPBasic, HTTPBasicCredentials, APIKeyHeader
from fastapi.responses import StreamingResponse
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
from pydantic import BaseModel
from rapidfuzz import process, fuzz
from dotenv import load_dotenv
import sys

from E3_E4_API_app import config


# Configuration du logger (mettre √ßa en d√©but de fichier)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


app = FastAPI(title="üé¨ Louve Movies API")

mlflow.set_tracking_uri(config.MLFLOW_TRACKING_URI)
mlflow.set_experiment("louve_movies_monitoring")

# ------------------------------
# Configuration BDD
# ------------------------------
DATABASE_URL = "mysql+pymysql://louve:%40Marley080922@mysql-louve.alwaysdata.net/louve_movies"
engine = create_engine(DATABASE_URL)

# ------------------------------
# Chargement des mod√®les ML
# ------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "model")

xgb_model = joblib.load(os.path.join(MODEL_DIR, "xgb_classifier_model.joblib"))
mlb = joblib.load(os.path.join(MODEL_DIR, "mlb_model.joblib"))
scaler_year = joblib.load(os.path.join(MODEL_DIR, "scaler_year.joblib"))
nn_full = joblib.load(os.path.join(MODEL_DIR, "nn_full.joblib"))
reco_vectorizer = joblib.load(os.path.join(MODEL_DIR, "reco_vectorizer.joblib"))
svd_full = joblib.load(os.path.join(MODEL_DIR, "svd_model.joblib"))
tfidf_matrix = joblib.load(os.path.join(MODEL_DIR, "tfidf_matrix_full.joblib"))
movie_index_df = pd.read_csv(os.path.join(MODEL_DIR, "movie_index.csv"))

# ------------------------------
# Chargement des films depuis BDD
# ------------------------------
with engine.connect() as conn:
    rows = conn.execute(
        text("SELECT movie_id, title, genres, release_year, synopsis, poster_url FROM movies")
    ).mappings().all()

movies = []
titles = []
genres_list_all = []
years = []

for r in rows:
    genres_list = r["genres"].split("|") if r["genres"] else []
    movies.append(dict(r))
    titles.append(r["title"])
    genres_list_all.append(genres_list)
    years.append(r["release_year"] if r["release_year"] else 2000)

# Encodage genres
genres_encoded_matrix = mlb.transform(genres_list_all)
years_scaled = scaler_year.transform(np.array([[y] for y in years]))

# Dict pour acc√®s rapide par titre
movies_dict = {movie["title"]: movie for movie in movies}

# ------------------------------
# Fonction d'interpr√©tation des scores
# ------------------------------
def interpret_score(score: float) -> str:
    pct = int(score * 100)
    if score >= 0.85:
        return f"üéØ Tr√®s forte recommandation ({pct}%)"
    elif score >= 0.7:
        return f"üëç Forte similarit√© ({pct}%)"
    elif score >= 0.5:
        return f"ü§î Moyennement similaire ({pct}%)"
    else:
        return f"‚ö†Ô∏è Faible similarit√© ({pct}%)"

# ------------------------------
# Tables plateformes
# ------------------------------
PLATFORM_TABLES = {
    "netflix": "netflix",
    "prime": "prime",
    "hulu": "hulu",
    "hbo": "hbo",
    "apple": "apple"
}

# ------------------------------
# Authentification HTTP Basic / Bearer
# ------------------------------
load_dotenv()

USERNAME: str = os.getenv("API_USERNAME")
PASSWORD: str = os.getenv("API_PASSWORD")
API_TOKEN: str = os.getenv("API_TOKEN")

security_basic = HTTPBasic(auto_error=False)
api_key_header = APIKeyHeader(name="Authorization", auto_error=False)

def verify_credentials(
    credentials: HTTPBasicCredentials = Depends(security_basic),
    api_key: str = Depends(api_key_header)
):
    # Mode 1 : Basic Auth
    if credentials and credentials.username == USERNAME and credentials.password == PASSWORD:
        return True

    # Mode 2 : Bearer Token
    if api_key and api_key == f"Bearer {API_TOKEN}":
        return True

    raise HTTPException(status_code=401, detail="Non autoris√©")

# ------------------------------
# Pydantic model
# ------------------------------
class RatingUpdate(BaseModel):
    title: str
    rating: float

# ------------------------------
# Route: Mise √† jour note utilisateur
# ------------------------------
@app.post("/update_rating", dependencies=[Depends(verify_credentials)])
async def update_rating(payload: RatingUpdate):
    title = payload.title
    rating = payload.rating

    if rating < 0 or rating > 10:
        raise HTTPException(status_code=400, detail="La note doit √™tre comprise entre 0 et 10.")

    try:
        with engine.begin() as conn:
            # V√©rifie si la colonne user_rating existe
            check_col = conn.execute(text("""
                SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = 'movies' AND COLUMN_NAME = 'user_rating'
            """)).fetchone()
            if not check_col:
                conn.execute(text("ALTER TABLE movies ADD COLUMN user_rating FLOAT"))

            # Mise √† jour
            result = conn.execute(
                text("UPDATE movies SET user_rating = :rating WHERE title = :title"),
                {"rating": rating, "title": title}
            )
            if result.rowcount == 0:
                raise HTTPException(status_code=404, detail=f"Film '{title}' non trouv√©.")

        return {"message": f"La note {rating} a √©t√© enregistr√©e pour le film '{title}'."}

    except HTTPException:  # ‚ö° On laisse passer le 404
        raise
    except SQLAlchemyError as e:
        raise HTTPException(status_code=500, detail=f"Erreur SQLAlchemy : {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur : {str(e)}")


# ------------------------------
# Route: Recommandation XGB personnalis√©e
# ------------------------------
@app.get("/recommend_xgb_personalized/{title}")
async def recommend_xgb_personalized(title: str, top_k: int = 5):
    try:
        idx = next(i for i, t in enumerate(titles) if t.lower() == title.lower())
    except StopIteration:
        raise HTTPException(status_code=404, detail="Film non trouv√©")

    # ------------------------
    # Start MLflow run
    # ------------------------
    run = mlflow.start_run(run_name=f"recommend_{title}")
    try:
        chosen_genres = set(genres_list_all[idx])
        vec = tfidf_matrix[idx].reshape(1, -1)
        cosine_sim = cosine_similarity(vec, tfidf_matrix).flatten()
        cosine_sim[idx] = -1
        candidate_indices = np.argsort(cosine_sim)[-50:][::-1]
        candidate_indices = [i for i in candidate_indices if chosen_genres & set(genres_list_all[i])]

        features_list = []
        candidate_titles = []

        for i in candidate_indices:
            svd_vec = svd_full.transform(tfidf_matrix[i])
            nn_distances, _ = nn_full.kneighbors(tfidf_matrix[i])
            neighbor_mean = (1 - nn_distances[0][1:]).mean()
            neighbor_max = (1 - nn_distances[0][1:]).max()
            neighbor_min = (1 - nn_distances[0][1:]).min()
            neighbor_std = (1 - nn_distances[0][1:]).std()

            feature_vec = np.hstack([
                svd_vec[0],
                genres_encoded_matrix[i],
                years_scaled[i],
                neighbor_mean,
                neighbor_max,
                neighbor_min,
                neighbor_std
            ])
            features_list.append(feature_vec)
            candidate_titles.append(titles[i])

        if not features_list:
            return []

        all_features = np.array(features_list)
        pred_scores = xgb_model.predict_proba(all_features)[:, 1]

        # Normalisation
        min_score, max_score = pred_scores.min(), pred_scores.max()
        pred_scores_scaled = (pred_scores - min_score) / (max_score - min_score) if max_score > min_score else np.zeros_like(pred_scores)

        top_indices = np.argsort(pred_scores_scaled)[::-1][:top_k]

        def sanitize_mlflow_key(title: str) -> str:
            key = unidecode(title)
            key = re.sub(r"[^0-9a-zA-Z_\-\.]", "_", key)
            return key.lower()

        top_recos_list = []

        for idx_top in top_indices:
            movie = movies_dict[candidate_titles[idx_top]]
            user_rating = movie.get("user_rating") or 0.0
            movie_rating = movie.get("rating") or 5.0
            pred_score_model = float(pred_scores_scaled[idx_top])
            score_final = 0.6 * pred_score_model + 0.25 * (user_rating / 10) + 0.15 * (movie_rating / 10)
            score_diff = abs(pred_score_model - (user_rating / 10))

            key_safe = sanitize_mlflow_key(movie["title"])

            # Logging MLflow
            mlflow.log_metric(f"pred_score_model_{key_safe}", pred_score_model)
            mlflow.log_metric(f"user_rating_{key_safe}", float(user_rating))
            mlflow.log_metric(f"movie_rating_{key_safe}", float(movie_rating))
            mlflow.log_metric(f"score_final_{key_safe}", score_final)
            mlflow.log_metric(f"score_diff_{key_safe}", score_diff)

            # Pr√©paration de la sortie
            genres_raw = movie.get("genres", [])
            if isinstance(genres_raw, str):
                genres_list = [g.strip() for g in genres_raw.replace(",", "|").split("|") if g.strip()]
            elif isinstance(genres_raw, list):
                genres_list = [g.strip() for g in genres_raw if g.strip()]
            else:
                genres_list = []

            top_recos_list.append({
                "title": movie["title"],
                "poster_url": movie.get("poster_url"),
                "releaseYear": movie.get("release_year"),
                "genres": genres_list,
                "synopsis": movie.get("synopsis"),
                "platforms": [],
                "pred_score": score_final,
                "movie_rating": movie_rating,
                "user_rating": user_rating,
                "score_diff": score_diff,
                "final_score": score_final
            })

        # Log global
        mlflow.log_param("input_title", title)
        mlflow.log_param("top_k", top_k)
        mlflow.log_metric("max_pred_score_model", float(pred_scores_scaled.max()))
        mlflow.log_metric("min_pred_score_model", float(pred_scores_scaled.min()))
        mlflow.log_text(str([r["title"] for r in top_recos_list]), "top_recommended_titles.txt")

        return top_recos_list

    finally:
        mlflow.end_run()
            
# ------------------------------
# Route: Fuzzy match
# ------------------------------
def normalize_text(text: str) -> str:
    text = text.strip()
    text = unicodedata.normalize("NFD", text).encode("ascii", "ignore").decode("utf-8")
    return text.lower()

@app.get("/fuzzy_match/{title}", dependencies=[Depends(verify_credentials)])
async def fuzzy_match(title: str):
    title_input = normalize_text(title.strip())
    choices = [(normalize_text(c), c) for c in movie_index_df["title"].tolist()]
    movies_dict_normalized = {normalize_text(k): v for k, v in movies_dict.items()}

    matches = process.extract(
        query=title_input,
        choices=[c[0] for c in choices],
        scorer=fuzz.WRatio,
        limit=20,
        score_cutoff=70
    )

    if not matches:
        raise HTTPException(status_code=404, detail="Aucune correspondance ‚â•70% trouv√©e.")

    seen_titles = set()
    filtered = []

    for match in matches:
        norm_title, score, idx = match
        original_title = choices[idx][1]
        if original_title in seen_titles:
            continue
        partial = fuzz.partial_ratio(title_input, norm_title)
        if partial >= 60:
            seen_titles.add(original_title)
            filtered.append({
                "title": original_title,
                "score": round(score),
                "movie_id": movies_dict_normalized.get(norm_title, {}).get("movie_id")
            })
        if len(filtered) >= 10:
            break

    if not filtered:
        raise HTTPException(status_code=404, detail="Aucune correspondance fiable trouv√©e.")

    return {"matches": filtered}


# ------------------------------
# Route: Genres uniques
# ------------------------------
@app.get("/genres/", dependencies=[Depends(verify_credentials)])
async def get_unique_genres():
    try:
        query = "SELECT genres FROM movies"
        all_genres = set()
        with engine.connect() as conn:
            result = conn.execute(text(query)).fetchall()
            for row in result:
                genres = row[0].split(",")
                all_genres.update([g.strip() for g in genres])
        return sorted(all_genres)
    except SQLAlchemyError as e:
        raise HTTPException(status_code=500, detail=f"Erreur SQLAlchemy : {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur : {str(e)}")

# ------------------------------
# Route: D√©tails d'un film
# ------------------------------
@app.get("/movie-details/{title}", dependencies=[Depends(verify_credentials)])
async def get_movie_details(title: str):
    try:
        query = f"""
        SELECT 
            m.movie_id,
            m.title AS movie_title,
            m.genres AS movie_genres,
            m.release_year,
            m.rating AS movie_rating,
            m.synopsis,
            m.poster_url,
            p.platform_name
        FROM movies m
        LEFT JOIN (
            SELECT 'Netflix' AS platform_name, title FROM netflix
            UNION
            SELECT 'Prime' AS platform_name, title FROM prime
            UNION
            SELECT 'Hulu' AS platform_name, title FROM hulu
            UNION
            SELECT 'HBO Max' AS platform_name, title FROM hbo
            UNION
            SELECT 'Apple' AS platform_name, title FROM apple
        ) p
        ON m.title = p.title
        WHERE LOWER(m.title) = LOWER(:title)
        """
        with engine.connect() as conn:
            result = conn.execute(text(query), {"title": title}).fetchall()

        if not result:
            raise HTTPException(status_code=404, detail="Film non trouv√©.")  # ‚úÖ avant d'acc√©der √† row[7]

        first = result[0]
        platforms = list({row[7] for row in result if row[7]})
        return {
            "movie_id": first[0],
            "title": first[1],
            "genres": first[2],
            "releaseYear": first[3],
            "rating": first[4],
            "synopsis": first[5],
            "poster_url": first[6],
            "platforms": platforms
        }
    
    except HTTPException:  # ‚úÖ ne pas avaler ton 404
        raise
    except SQLAlchemyError as e:
        raise HTTPException(status_code=500, detail=f"Erreur SQLAlchemy : {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur : {str(e)}")

# ------------------------------
# Route: Films al√©atoires par genre et plateforme
# ------------------------------
@app.get("/random_movies/", dependencies=[Depends(verify_credentials)])
async def get_random_movies(genre: str, platforms: List[str] = Query(...), limit: int = 10):
    try:
        selected_platforms = [p.lower() for p in platforms if p.lower() in PLATFORM_TABLES]
        if not selected_platforms:
            raise HTTPException(status_code=400, detail="Aucune plateforme valide s√©lectionn√©e.")

        movies = []
        with engine.connect() as conn:
            for platform in selected_platforms:
                query = f"""
                SELECT m.title, m.synopsis, m.poster_url, m.genres, '{platform}' AS platform
                FROM movies m
                JOIN {platform} p ON m.title = p.title
                WHERE FIND_IN_SET(:genre, m.genres)
                """
                result = conn.execute(text(query), {"genre": genre}).fetchall()
                for row in result:
                    movies.append({
                        "title": row[0],
                        "synopsis": row[1],
                        "poster_url": row[2],
                        "genres": row[3],
                        "platform": row[4]
                    })

        if not movies:
            raise HTTPException(status_code=404, detail="Aucun film trouv√© pour ce genre et ces plateformes.")

        return random.sample(movies, min(limit, len(movies)))

    except HTTPException:  # ‚ö° prot√©g√© (400 & 404)
        raise
    except SQLAlchemyError as e:
        raise HTTPException(status_code=500, detail=f"Erreur SQLAlchemy : {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur serveur : {str(e)}")


# ------------------------------
# Route: T√©l√©chargement CSV
# ------------------------------
@app.get("/download-movie-details/", dependencies=[Depends(verify_credentials)])
async def download_movie_details():
    try:
        query = f"""
        SELECT 
            m.movie_id,
            m.title AS movie_title,
            m.genres AS movie_genres,
            m.release_year,
            m.rating AS movie_rating,
            m.synopsis,
            m.poster_url,
            COALESCE(p.platform_name, 'Not available') AS platform_name
        FROM movies m
        LEFT JOIN (
            SELECT 'Netflix' AS platform_name, title FROM netflix
            UNION
            SELECT 'Prime' AS platform_name, title FROM prime
            UNION
            SELECT 'Hulu' AS platform_name, title FROM hulu
            UNION
            SELECT 'HBO Max' AS platform_name, title FROM hbo
            UNION
            SELECT 'Apple' AS platform_name, title FROM apple
        ) p
        ON m.title = p.title
        ORDER BY m.title;
        """
        with engine.connect() as conn:
            result = conn.execute(text(query))
            rows = result.fetchall()

        if not rows:
            raise HTTPException(status_code=404, detail="Aucune donn√©e √† t√©l√©charger.")

        csv_file = io.StringIO()
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow([col for col in result.keys()])
        csv_writer.writerows(rows)
        csv_file.seek(0)

        return StreamingResponse(
            csv_file,
            media_type="text/csv",
            headers={"Content-Disposition": "attachment; filename=movie_details.csv"}
        )
    except SQLAlchemyError as e:
        raise HTTPException(status_code=500, detail=f"Erreur SQLAlchemy : {str(e)}")
    except Exception as e:

        raise HTTPException(status_code=500, detail=f"Erreur serveur : {str(e)}")






